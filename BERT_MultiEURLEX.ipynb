{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLB3I4FKZ5Lr"
   },
   "source": [
    "# Fine-tuning BERT (and friends) for multi-label text classification\n",
    "\n",
    "In this notebook, we are going to fine-tune BERT to predict one or more labels for a given piece of text. Note that this notebook illustrates how to fine-tune a bert-base-uncased model, but you can also fine-tune a RoBERTa, DeBERTa, DistilBERT, CANINE, ... checkpoint in the same way. \n",
    "\n",
    "All of those work in the same way: they add a linear layer on top of the base model, which is used to produce a tensor of shape (batch_size, num_labels), indicating the unnormalized scores for a number of labels for every example in the batch.\n",
    "\n",
    "\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "First, we install the libraries which we'll use: HuggingFace Transformers and Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wxY3x-ZZz8h",
    "outputId": "c7ef3a90-a6c7-477a-f951-b7a1244868b0"
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIH9NP0MZ6-O"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Next, let's download a multi-label text classification dataset from the [hub](https://huggingface.co/).\n",
    "\n",
    "At the time of writing, I picked a random one as follows:   \n",
    "\n",
    "* first, go to the \"datasets\" tab on huggingface.co\n",
    "* next, select the \"multi-label-classification\" tag on the left as well as the the \"1k<10k\" tag (fo find a relatively small dataset).\n",
    "\n",
    "Note that you can also easily load your local data (i.e. csv files, txt files, Parquet files, JSON, ...) as explained [here](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "g5MNOwT4xKqH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "a1460512c5f1452e90bc943d2dbe1f1b",
      "16538a192cb7482aabbaae4b633802e5",
      "c5d1139ec488496bb5ed2add54bbbbc3",
      "b340d400a5a54da394eb030a89db63cd",
      "3fba3a22993c4aedb1cf3bf745291cf7",
      "a4d8b4662a5b4feba472c590f19a0f46",
      "5c4a113498a0434c905c86ff73b2abea",
      "1cd0041bd03f4028b421f7a44fc3c05e",
      "62471587c0c44a5182c4cb3033a78e19",
      "c986f7a7cc7547adbc76007edc044c2c",
      "4a372b0842fa4a3e9aaf869bc2250cfc"
     ]
    },
    "id": "sd1LiXGjZ420",
    "outputId": "ccfa6f9b-b217-48f7-d7b5-a764b77e31fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/davo/.cache/huggingface/datasets/nlpaueb___multi_eurlex/en/1.0.0/1addee7110a20c2b01cc3de89456786482e4eea1d2ead0bea3d5383b16cc9fce)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757de129766d481c840a51442cb45994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nlpaueb/multi_eurlex\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCL02vQgxYTO"
   },
   "source": [
    "As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qh1zti-xbYiq"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.rename_column(\"labels\", \"old_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBDCKGHmVq0T",
    "outputId": "8734b307-2eb5-44d4-d362-7394e742ec78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['celex_id', 'text', 'old_labels'],\n",
       "        num_rows: 11000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['celex_id', 'text', 'old_labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['celex_id', 'text', 'old_labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgS0wMWExcqP"
   },
   "source": [
    "Let's check the first example of the training split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unjuTtKUjZI3",
    "outputId": "dbba328e-10fb-438d-9e32-42856734ebfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'celex_id': '32003R1012',\n",
       " 'text': 'Commission Regulation (EC) No 1012/2003\\nof 12 June 2003\\namending for the 19th time Council Regulation (EC) No 881/2002 imposing certain specific restrictive measures directed against certain persons and entities associated with Usama bin Laden, the Al-Qaida network and the Taliban, and repealing Council Regulation (EC) No 467/2001\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Council Regulation (EC) No 881/2002 of 27 May 2002 imposing certain specific restrictive measures directed against certain persons and entities associated with Usama bin Laden, the Al-Qaida network and the Taliban, and repealing Council Regulation (EC) No 467/2001 prohibiting the export of certain goods and services to Afghanistan, strengthening the flight ban and extending the freeze of funds and other financial resources in respect of the Taliban of Afghanistan(1), as last amended by Commission Regulation (EC) No 866/2003(2), and in particular Article 7(1), first indent, thereof,\\nWhereas:\\n(1) Annex I to Regulation (EC) No 881/2002 lists the persons, groups and entities covered by the freezing of funds and economic resources under that Regulation.\\n(2) On 10 June 2003, the Sanctions Committee decided to amend the list of persons, groups and entities to whom the freezing of funds and economic resources should apply. Therefore, Annex I should be amended accordingly,\\nHAS ADOPTED THIS REGULATION:\\nArticle 1\\nAnnex I to Regulation (EC) No 881/2002 is hereby amended in accordance with the Annex to this Regulation.\\nArticle 2\\nThis Regulation shall enter into force on the day following that of its publication in the Official Journal of the European Union.\\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.\\nDone at Brussels, 12 June 2003.',\n",
       " 'old_labels': [2, 5, 10, 8, 3, 18, 15]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DV0Rtetxgd4"
   },
   "source": [
    "The dataset consists of tweets, labeled with one or more emotions. \n",
    "\n",
    "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5vZhQpvkE8s",
    "outputId": "f7278c2b-a522-497f-a3bc-ffd7da9ec376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['social questions',\n",
       " 'industry',\n",
       " 'finance',\n",
       " 'trade',\n",
       " 'business and competition',\n",
       " 'international relations',\n",
       " 'agriculture, forestry and fisheries',\n",
       " 'production, technology and research',\n",
       " 'transport',\n",
       " 'employment and working conditions',\n",
       " 'politics',\n",
       " 'law',\n",
       " 'education and communications',\n",
       " 'international organisations',\n",
       " 'energy',\n",
       " 'EUROPEAN UNION',\n",
       " 'science',\n",
       " 'agri-foodstuffs',\n",
       " 'geography',\n",
       " 'economics',\n",
       " 'environment']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['social questions',\n",
    " 'industry',\n",
    " 'finance',\n",
    " 'trade',\n",
    " 'business and competition',\n",
    " 'international relations',\n",
    " 'agriculture, forestry and fisheries',\n",
    " 'production, technology and research',\n",
    " 'transport',\n",
    " 'employment and working conditions',\n",
    " 'politics',\n",
    " 'law',\n",
    " 'education and communications',\n",
    " 'international organisations',\n",
    " 'energy',\n",
    " 'EUROPEAN UNION',\n",
    " 'science',\n",
    " 'agri-foodstuffs',\n",
    " 'geography',\n",
    " 'economics',\n",
    " 'environment']\n",
    "id2label = {0: 'social questions',\n",
    " 1: 'industry',\n",
    " 2: 'finance',\n",
    " 3: 'trade',\n",
    " 4: 'business and competition',\n",
    " 5: 'international relations',\n",
    " 6: 'agriculture, forestry and fisheries',\n",
    " 7: 'production, technology and research',\n",
    " 8: 'transport',\n",
    " 9: 'employment and working conditions',\n",
    " 10: 'politics',\n",
    " 11: 'law',\n",
    " 12: 'education and communications',\n",
    " 13: 'international organisations',\n",
    " 14: 'energy',\n",
    " 15: 'EUROPEAN UNION',\n",
    " 16: 'science',\n",
    " 17: 'agri-foodstuffs',\n",
    " 18: 'geography',\n",
    " 19: 'economics',\n",
    " 20: 'environment'}\n",
    "label2id = {'social questions': 0,\n",
    " 'industry': 1,\n",
    " 'finance': 2,\n",
    " 'trade': 3,\n",
    " 'business and competition': 4,\n",
    " 'international relations': 5,\n",
    " 'agriculture, forestry and fisheries': 6,\n",
    " 'production, technology and research': 7,\n",
    " 'transport': 8,\n",
    " 'employment and working conditions': 9,\n",
    " 'politics': 10,\n",
    " 'law': 11,\n",
    " 'education and communications': 12,\n",
    " 'international organisations': 13,\n",
    " 'energy': 14,\n",
    " 'EUROPEAN UNION': 15,\n",
    " 'science': 16,\n",
    " 'agri-foodstuffs': 17,\n",
    " 'geography': 18,\n",
    " 'economics': 19,\n",
    " 'environment': 20}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ3Teyjmank2"
   },
   "source": [
    "## Preprocess data\n",
    "\n",
    "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
    "\n",
    "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TEQebh4p22NB"
   },
   "outputs": [],
   "source": [
    "def numbers_to_classes(l=[0,10,20]):\n",
    "  zero_cl = [0.0] * 21\n",
    "  for i in l:\n",
    "    zero_cl[i] = 1.0\n",
    "  \n",
    "  return np.array(zero_cl, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPmxZtbb31EY",
    "outputId": "3aefa237-2631-4547-9e32-1b759ecaaf1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_to_classes(l=[0,10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AFWlSsbZaRLc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=300)\n",
    "  # add labels\n",
    "\n",
    "  labels_matrix = []\n",
    "  for r in examples[\"old_labels\"]:\n",
    "    labels_matrix.append(np.array(numbers_to_classes(r), dtype=float))\n",
    "  # print(labels_matrix)\n",
    "  encoding[\"labels\"] = np.array(labels_matrix, dtype=float)\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "referenced_widgets": [
      "afccdd194fec4ede9c65846cb0781d68",
      "45746650248241e1942e8809c94b06a4",
      "8901c5d6493f42fcb90992f2db6e402b",
      "bca5bf694e1e44ce8be4e273f1c53ce2",
      "68abbd466ba64e8088b80c65bc0adb63",
      "c4f502d0988648d7996e1a55108a0fbe",
      "669f35f7f8fc4ad3b510972b19c69a3c",
      "d8f166d45b1c41d4b29e6cf63f5ed29d",
      "439053c658424b2f93a3f383b8d0af95",
      "3ab9eb89743744a9aa717dd82a129201",
      "73faac60485a45ef9234d3464b0b1c74",
      "b4a289a28e694652b8e8d6d518314f63",
      "d9d01b97af884a198cfea00559c1f199",
      "431cea2a52604dd4910542e1fac2d045",
      "66f61081b48d4f7785e1d150c3cb9a07",
      "91ae61f382f14883b9337f17dc8e7267",
      "c2582028d3ed41b9a32bc31df53a16dd",
      "df37c400a9014907a887fce2a5fc7042",
      "985db658d3614f1aabb57d173732f678",
      "ec1f581ed92148b5bee7ce0441f8d883",
      "686b34d4d82d4a5691fc6d4dfc51d0bb",
      "0571ec3060ae43b3811de5ccc85c9385",
      "4082b8176278445997a2453706b7b101",
      "d0b9d1bfa51546f8a775488417d3a684",
      "6c8eabd8b41f4cc4934c9c1f060026c6",
      "ca0cf0adf2bb4653b04cd85c25d14a3e",
      "1c66085d538641d28d132d723c053fa2",
      "adeb54cc7fa44b1f948595fb1e887926",
      "b3ff5b01b690434b8763f056165078c0",
      "9e9849e134bd42cba950405613d6cee6",
      "890e461de2bb4d2e902ca483b9c7b203",
      "fb31a2d5e23b465bb6abbdda03a17f02",
      "fe78160e25464f1589b6e09917c0f3f5"
     ]
    },
    "id": "i4ENBTdulBEI",
    "outputId": "a3266c2d-2dcc-4269-975d-9684cf05e941"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_data at 0x7fb8e34060e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bb6a7f28cf4900a32a1808510812d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3cc1f9d7c74a49b2acac79ebdc9476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c679383fe97649288aaa1eaf46e7aa6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "840d2ddc-18f9-44a9-a2e9-fe2ea77d7d3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['celex_id', 'text', 'old_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "b9068821-7e2c-44db-c962-2e4a717b5534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] commission regulation ( ec ) no 1012 / 2003 of 12 june 2003 amending for the 19th time council regulation ( ec ) no 881 / 2002 imposing certain specific restrictive measures directed against certain persons and entities associated with usama bin laden, the al - qaida network and the taliban, and repealing council regulation ( ec ) no 467 / 2001 the commission of the european communities, having regard to the treaty establishing the european community, having regard to council regulation ( ec ) no 881 / 2002 of 27 may 2002 imposing certain specific restrictive measures directed against certain persons and entities associated with usama bin laden, the al - qaida network and the taliban, and repealing council regulation ( ec ) no 467 / 2001 prohibiting the export of certain goods and services to afghanistan, strengthening the flight ban and extending the freeze of funds and other financial resources in respect of the taliban of afghanistan ( 1 ), as last amended by commission regulation ( ec ) no 866 / 2003 ( 2 ), and in particular article 7 ( 1 ), first indent, thereof, whereas : ( 1 ) annex i to regulation ( ec ) no 881 / 2002 lists the persons, groups and entities covered by the freezing of funds and economic resources under that regulation. ( 2 ) on 10 june 2003, the sanctions committee decided to amend the list of persons, groups and entities to whom the freezing of funds and economic resources should apply. therefore, annex i should be amended accordingly, has [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Dx95t2o6N9",
    "outputId": "d70411e3-29fe-4e72-d156-e39367abe460"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finance',\n",
       " 'trade',\n",
       " 'international relations',\n",
       " 'transport',\n",
       " 'politics',\n",
       " 'EUROPEAN UNION',\n",
       " 'geography']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgpKXDfvKBxn"
   },
   "source": [
    "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Lk6Cq9duKBkA"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5qSmCgWefWs"
   },
   "source": [
    "## Define model\n",
    "\n",
    "Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
    "\n",
    "This is also printed by the warning.\n",
    "\n",
    "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XPL1Z_RegBF",
    "outputId": "5e2f251f-82b9-4f0e-a90d-8054bd860044"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjJGEXShp7te"
   },
   "source": [
    "## Train the model!\n",
    "\n",
    "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things: \n",
    "\n",
    "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
    "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "K5a8_vIKqr7P"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dR2GmpvDqbuZ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_v2fPFFJ3-v"
   },
   "source": [
    "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "797b2WHJqUgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxNo4_TsvzDm"
   },
   "source": [
    "Let's verify a batch as well as a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IlOgGiojuWwG",
    "outputId": "b458483e-0d83-462c-fb34-39e5cc70c1f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9wPaLZ7AXnV",
    "outputId": "9c79f006-3391-447d-e3d2-42456356486c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoded_dataset['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  3222,  7816,  1006, 14925,  1007,  2053,  7886,  2475,  1013,\n",
       "         2494,  1997,  2260,  2238,  2494, 27950,  2075,  2005,  1996,  3708,\n",
       "         2051,  2473,  7816,  1006, 14925,  1007,  2053,  6070,  2487,  1013,\n",
       "         2526, 16625,  3056,  3563, 25986,  5761,  2856,  2114,  3056,  5381,\n",
       "         1998, 11422,  3378,  2007,  3915,  2863,  8026, 14887,  1010,  1996,\n",
       "         2632,  1011,  1053, 14326,  2050,  2897,  1998,  1996, 16597,  1010,\n",
       "         1998, 21825,  2075,  2473,  7816,  1006, 14925,  1007,  2053,  4805,\n",
       "         2581,  1013,  2541,  1996,  3222,  1997,  1996,  2647,  4279,  1010,\n",
       "         2383,  7634,  2000,  1996,  5036,  7411,  1996,  2647,  2451,  1010,\n",
       "         2383,  7634,  2000,  2473,  7816,  1006, 14925,  1007,  2053,  6070,\n",
       "         2487,  1013,  2526,  1997,  2676,  2089,  2526, 16625,  3056,  3563,\n",
       "        25986,  5761,  2856,  2114,  3056,  5381,  1998, 11422,  3378,  2007,\n",
       "         3915,  2863,  8026, 14887,  1010,  1996,  2632,  1011,  1053, 14326,\n",
       "         2050,  2897,  1998,  1996, 16597,  1010,  1998, 21825,  2075,  2473,\n",
       "         7816,  1006, 14925,  1007,  2053,  4805,  2581,  1013,  2541, 26325,\n",
       "         1996,  9167,  1997,  3056,  5350,  1998,  2578,  2000,  7041,  1010,\n",
       "        16003,  1996,  3462,  7221,  1998,  8402,  1996, 13184,  1997,  5029,\n",
       "         1998,  2060,  3361,  4219,  1999,  4847,  1997,  1996, 16597,  1997,\n",
       "         7041,  1006,  1015,  1007,  1010,  2004,  2197, 13266,  2011,  3222,\n",
       "         7816,  1006, 14925,  1007,  2053,  6564,  2575,  1013,  2494,  1006,\n",
       "         1016,  1007,  1010,  1998,  1999,  3327,  3720,  1021,  1006,  1015,\n",
       "         1007,  1010,  2034, 27427,  4765,  1010, 21739,  1010,  6168,  1024,\n",
       "         1006,  1015,  1007, 17827,  1045,  2000,  7816,  1006, 14925,  1007,\n",
       "         2053,  6070,  2487,  1013,  2526,  7201,  1996,  5381,  1010,  2967,\n",
       "         1998, 11422,  3139,  2011,  1996, 12809,  1997,  5029,  1998,  3171,\n",
       "         4219,  2104,  2008,  7816,  1012,  1006,  1016,  1007,  2006,  2184,\n",
       "         2238,  2494,  1010,  1996, 17147,  2837,  2787,  2000, 27950,  1996,\n",
       "         2862,  1997,  5381,  1010,  2967,  1998, 11422,  2000,  3183,  1996,\n",
       "        12809,  1997,  5029,  1998,  3171,  4219,  2323,  6611,  1012,  3568,\n",
       "         1010, 17827,  1045,  2323,  2022, 13266, 11914,  1010,  2038,   102])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "909a444f-ae12-4402-fdc4-925f5bcdeb67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7208, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), logits=tensor([[ 0.6385,  0.0477,  0.0472,  1.1657, -0.4790,  0.0196, -0.0694,  0.7476,\n",
       "          0.0020,  0.1310,  0.8710,  0.0487, -0.0142,  0.1482,  0.9864, -0.4025,\n",
       "          0.2935, -0.0678,  0.3514, -0.4830,  0.0882]],\n",
       "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train']['labels'][0].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-X2brZcv0X6"
   },
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "chq_3nUz73ib"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davo/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/davo/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/davo/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/davo/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "KXmFds8js6P8",
    "outputId": "06ea76ea-8c6b-4ab1-abcc-26991cd26aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/davo/anaconda3/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 11000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34400\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdyada\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/davo/Desktop/wandb/run-20230330_002715-2dxuv0qd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dyada/huggingface/runs/2dxuv0qd\" target=\"_blank\">bert-finetuned-sem_eval-english</a></strong> to <a href=\"https://wandb.ai/dyada/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34400' max='34400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34400/34400 3:48:01, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>0.242892</td>\n",
       "      <td>0.699967</td>\n",
       "      <td>0.782958</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.206527</td>\n",
       "      <td>0.768737</td>\n",
       "      <td>0.831223</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.194616</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.839774</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>0.192727</td>\n",
       "      <td>0.778690</td>\n",
       "      <td>0.842318</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.786677</td>\n",
       "      <td>0.848659</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.196106</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.850874</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.795419</td>\n",
       "      <td>0.861934</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.202194</td>\n",
       "      <td>0.796569</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.213390</td>\n",
       "      <td>0.778095</td>\n",
       "      <td>0.852925</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.208805</td>\n",
       "      <td>0.791625</td>\n",
       "      <td>0.857577</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.217681</td>\n",
       "      <td>0.784810</td>\n",
       "      <td>0.854657</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.221487</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.862899</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.226746</td>\n",
       "      <td>0.792913</td>\n",
       "      <td>0.863040</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.233798</td>\n",
       "      <td>0.795089</td>\n",
       "      <td>0.865437</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.235237</td>\n",
       "      <td>0.799379</td>\n",
       "      <td>0.868013</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.244637</td>\n",
       "      <td>0.793863</td>\n",
       "      <td>0.863008</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.248258</td>\n",
       "      <td>0.798030</td>\n",
       "      <td>0.867851</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.259636</td>\n",
       "      <td>0.797767</td>\n",
       "      <td>0.869904</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.257654</td>\n",
       "      <td>0.797916</td>\n",
       "      <td>0.867638</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.269313</td>\n",
       "      <td>0.798595</td>\n",
       "      <td>0.868550</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.266672</td>\n",
       "      <td>0.797467</td>\n",
       "      <td>0.867522</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.281055</td>\n",
       "      <td>0.794618</td>\n",
       "      <td>0.864213</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.282874</td>\n",
       "      <td>0.796860</td>\n",
       "      <td>0.868195</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.795327</td>\n",
       "      <td>0.866234</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.290174</td>\n",
       "      <td>0.796325</td>\n",
       "      <td>0.869714</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.291426</td>\n",
       "      <td>0.800953</td>\n",
       "      <td>0.870820</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.302915</td>\n",
       "      <td>0.795177</td>\n",
       "      <td>0.864172</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.303814</td>\n",
       "      <td>0.799665</td>\n",
       "      <td>0.871319</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.302242</td>\n",
       "      <td>0.803380</td>\n",
       "      <td>0.870794</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.301047</td>\n",
       "      <td>0.803156</td>\n",
       "      <td>0.870551</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.312858</td>\n",
       "      <td>0.800503</td>\n",
       "      <td>0.871259</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.317011</td>\n",
       "      <td>0.801676</td>\n",
       "      <td>0.872024</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.315521</td>\n",
       "      <td>0.805072</td>\n",
       "      <td>0.874660</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.323364</td>\n",
       "      <td>0.798766</td>\n",
       "      <td>0.869240</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.325743</td>\n",
       "      <td>0.799605</td>\n",
       "      <td>0.867886</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.325198</td>\n",
       "      <td>0.801235</td>\n",
       "      <td>0.870430</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.327250</td>\n",
       "      <td>0.800503</td>\n",
       "      <td>0.871444</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.330169</td>\n",
       "      <td>0.802740</td>\n",
       "      <td>0.872391</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.331809</td>\n",
       "      <td>0.804436</td>\n",
       "      <td>0.872269</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.335933</td>\n",
       "      <td>0.803288</td>\n",
       "      <td>0.873644</td>\n",
       "      <td>0.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.340997</td>\n",
       "      <td>0.801459</td>\n",
       "      <td>0.870673</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.343874</td>\n",
       "      <td>0.801404</td>\n",
       "      <td>0.870381</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.342766</td>\n",
       "      <td>0.803152</td>\n",
       "      <td>0.870921</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.346337</td>\n",
       "      <td>0.800672</td>\n",
       "      <td>0.870655</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>0.803345</td>\n",
       "      <td>0.873566</td>\n",
       "      <td>0.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.347617</td>\n",
       "      <td>0.802467</td>\n",
       "      <td>0.871487</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.348267</td>\n",
       "      <td>0.801626</td>\n",
       "      <td>0.870993</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.350830</td>\n",
       "      <td>0.800673</td>\n",
       "      <td>0.870470</td>\n",
       "      <td>0.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.349831</td>\n",
       "      <td>0.802918</td>\n",
       "      <td>0.871602</td>\n",
       "      <td>0.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.350415</td>\n",
       "      <td>0.802916</td>\n",
       "      <td>0.871787</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-688\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-688/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-688/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-688/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-688/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1376\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1376/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1376/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1376/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1376/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2064\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2064/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2064/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2064/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2064/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2752\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2752/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2752/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2752/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2752/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3440\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3440/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3440/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3440/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3440/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4128\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4128/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4128/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4128/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4128/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4816\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4816/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4816/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4816/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4816/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5504\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5504/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5504/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5504/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5504/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-6192\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-6192/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-6192/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-6192/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-6192/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-6880\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-6880/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-6880/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-6880/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-6880/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-7568\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-7568/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-7568/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-7568/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-7568/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-8256\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-8256/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-8256/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-8256/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-8256/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-8944\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-8944/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-8944/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-8944/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-8944/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-9632\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-9632/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-9632/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-9632/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-9632/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-10320\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-10320/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-10320/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-10320/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-10320/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-11008\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-11008/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-11008/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-11008/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-11008/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-11696\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-11696/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-11696/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-11696/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-11696/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-12384\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-12384/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-12384/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-12384/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-12384/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-13072\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-13072/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-13072/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-13072/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-13072/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-13760\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-13760/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-13760/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-13760/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-13760/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-14448\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-14448/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-14448/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-14448/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-14448/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-15136\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-15136/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-15136/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-15136/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-15136/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-15824\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-15824/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-15824/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-15824/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-15824/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-16512\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-16512/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-16512/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-16512/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-16512/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-17200\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-17200/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-17200/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-17200/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-17200/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-17888\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-17888/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-17888/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-17888/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-17888/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-18576\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-18576/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-18576/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-18576/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-18576/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-19264\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-19264/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-19264/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-19264/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-19264/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-19952\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-19952/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-19952/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-19952/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-19952/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-20640\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-20640/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-20640/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-20640/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-20640/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-21328\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-21328/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-21328/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-21328/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-21328/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-22016\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-22016/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-22016/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-22016/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-22016/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-22704\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-22704/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-22704/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-22704/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-22704/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-23392\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-23392/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-23392/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-23392/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-23392/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-24080\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-24080/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-24080/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-24080/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-24080/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-24768\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-24768/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-24768/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-24768/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-24768/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-25456\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-25456/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-25456/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-25456/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-25456/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-26144\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-26144/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-26144/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-26144/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-26144/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-26832\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-26832/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-26832/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-26832/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-26832/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-27520\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-27520/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-27520/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-27520/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-27520/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-28208\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-28208/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-28208/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-28208/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-28208/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-28896\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-28896/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-28896/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-28896/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-28896/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-29584\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-29584/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-29584/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-29584/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-29584/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-30272\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-30272/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-30272/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-30272/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-30272/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-30960\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-30960/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-30960/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-30960/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-30960/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-31648\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-31648/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-31648/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-31648/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-31648/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-32336\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-32336/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-32336/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-32336/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-32336/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-33024\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-33024/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-33024/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-33024/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-33024/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-33712\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-33712/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-33712/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-33712/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-33712/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-34400\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-34400/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-34400/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-34400/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-34400/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-22704 (score: 0.8050717570015328).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34400, training_loss=0.030621453419674274, metrics={'train_runtime': 13689.2812, 'train_samples_per_second': 40.177, 'train_steps_per_second': 2.513, 'total_flos': 8.480611359e+16, 'train_loss': 0.030621453419674274, 'epoch': 50.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiloh9eMK91o"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "After training, we evaluate our model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "cMlebJ83LRYG",
    "outputId": "b18102e7-2198-4beb-c874-d39636f740ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, celex_id, old_labels. If text, celex_id, old_labels are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.31552064418792725,\n",
       " 'eval_f1': 0.8050717570015328,\n",
       " 'eval_roc_auc': 0.8746604597581796,\n",
       " 'eval_accuracy': 0.276,\n",
       " 'eval_runtime': 7.1405,\n",
       " 'eval_samples_per_second': 140.045,\n",
       " 'eval_steps_per_second': 8.823,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nmvJp0pLq-3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's test the model on a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COUNCIL REGULATION (EU) No 1390/2013\\nof 16 December 2013\\non the allocation of fishing opportunities under the Protocol agreed between the European Union and the Union of the Comoros setting out the fishing opportunities and financial contribution provided for in the Fisheries Partnership Agreement currently in force between the two parties\\nTHE COUNCIL OF THE EUROPEAN UNION,\\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Article 43(3) thereof,\\nHaving regard to the proposal from the European Commission,\\nWhereas:\\n(1)\\nOn 5 October 2006, the Council approved the conclusion of the Partnership Agreement in the fisheries sector between the European Community and the Union of the Comoros (the Partnership Agreement) by adopting Regulation (EC) No 1563/2006 (1).\\n(2)\\nThe European Union negotiated with the Union of the Comoros a new Protocol to the Partnership Agreement granting vessels of the European Union fishing opportunities in Comoros waters.\\n(3)\\nOn 16 December 2013, the Council adopted Decision 2013/786/EU (2) on the signing and provisional application of the new Protocol.\\n(4)\\nThe fishing opportunities among the Member States should be allocated for the period of application of the new Protocol.\\n(5)\\nCouncil Regulation (EC) No 1006/2008 (3) provides that the Commission is to inform the Member States concerned if it appears that the fishing opportunities allocated to the European Union under the new Protocol are not fully exhausted. If no reply is received within a time limit to be set by the Council, this will be considered as confirmation that the vessels of the Member State concerned are not making full use of their fishing opportunities during the period in question. That time limit should be set.\\n(6)\\nTo ensure that vessels of the European Union can continue their fishing activities, the new Protocol provides for its application by the Parties on a provisional basis as from 1 January 2014. This Regulation should therefore apply from the provisional application of the new Protocol,\\nHAS ADOPTED THIS REGULATION:\\nArticle 1\\n1. The fishing opportunities established under the Protocol agreed between the European Union and the Union of the Comoros setting out the fishing opportunities and financial contribution provided for by the Fisheries Partnership Agreement between the two parties currently in force (the Protocol) shall be allocated among the Member States as follows:\\n(a)\\n42 tuna seiners:\\n-\\nSpain: 21 vessels\\n-\\nFrance: 21 vessels\\n(b)\\n20 surface longliners:\\n-\\nSpain: 8 vessels\\n-\\nFrance: 9 vessels\\n-\\nPortugal: 3 vessels\\n2. Regulation (EC) No 1006/2008 shall apply without prejudice to the Protocol or the Fisheries Partnership Agreement.\\n3. The time limit within which the Member States are to confirm that they are not fully exhausting the fishing opportunities granted to them under the Fisheries Partnership Agreement, as provided by Article 10(1) of Regulation (EC) No 1006/2008, shall be set at ten working days from the date on which the Commission informs them that their fishing opportunities have not been fully exhausted.\\nArticle 2\\nThis Regulation shall enter into force on the day following its publication in the Official Journal of the European Union.\\nIt shall apply from 1 January 2014.\\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.\\nDone at Brussels, 16 December 2013.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below you can observe a few examples with their predicted outputs for Bert model and what actually was true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(text, thres_prob=0.5):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "    outputs = trainer.model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    predictions[np.where(probs >= thres_prob)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_labels(text=dataset['test']['text'][0][:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels>>> ['international relations', 'agriculture, forestry and fisheries', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('predicted_labels>>>', predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels>>> ['international relations', 'agriculture, forestry and fisheries', 'EUROPEAN UNION', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('true labels>>>', [id2label[idx] for idx, label in enumerate(numbers_to_classes(dataset['test']['old_labels'][0])) if label == 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the case above Bert model was able to pick up all true categories but missed 'EUROPEAN UNION'  category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_labels(text=dataset['test']['text'][100][:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels>>> ['trade', 'agriculture, forestry and fisheries', 'agri-foodstuffs', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('predicted_labels>>>', predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels>>> ['trade', 'agriculture, forestry and fisheries', 'production, technology and research', 'agri-foodstuffs', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('true labels>>>', [id2label[idx] for idx, label in enumerate(numbers_to_classes(dataset['test']['old_labels'][100])) if label == 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the case above Bert model was able to pick up all true categories but missed 'production, technology and research'  category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_labels(text=dataset['test']['text'][1000][:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels>>> ['international relations', 'EUROPEAN UNION', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('predicted_labels>>>', predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels>>> ['trade', 'international relations', 'EUROPEAN UNION', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('true labels>>>', [id2label[idx] for idx, label in enumerate(numbers_to_classes(dataset['test']['old_labels'][1000])) if label == 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the case above Bert model was able to pick up all true categories but also missed 'trade' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_labels(text=dataset['test']['text'][1000][:512], thres_prob=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels>>> ['trade', 'international relations', 'EUROPEAN UNION', 'geography', 'economics']\n"
     ]
    }
   ],
   "source": [
    "print('predicted_labels>>>', predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels>>> ['trade', 'international relations', 'EUROPEAN UNION', 'geography']\n"
     ]
    }
   ],
   "source": [
    "print('true labels>>>', [id2label[idx] for idx, label in enumerate(numbers_to_classes(dataset['test']['old_labels'][1000])) if label == 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if we reduce threshold to 1% for accepting  a label as related to the class then we capture trade but mislabel economics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_labels(text=dataset['test']['text'][1111][:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels>>> ['social questions', 'trade', 'agri-foodstuffs']\n"
     ]
    }
   ],
   "source": [
    "print('predicted_labels>>>', predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels>>> ['trade', 'agri-foodstuffs']\n"
     ]
    }
   ],
   "source": [
    "print('true labels>>>', [id2label[idx] for idx, label in enumerate(numbers_to_classes(dataset['test']['old_labels'][1111])) if label == 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we capture trade and agri-foodstuffs but mislabel social questions category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_labels(text=dataset['test']['text'][2230][:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_labels>>> ['trade', 'agri-foodstuffs']\n"
     ]
    }
   ],
   "source": [
    "print('predicted_labels>>>', predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels>>> ['trade', 'agri-foodstuffs']\n"
     ]
    }
   ],
   "source": [
    "print('true labels>>>', [id2label[idx] for idx, label in enumerate(numbers_to_classes(dataset['test']['old_labels'][2230])) if label == 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on output above we can observe that Bert returns also fully correct list of labels as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on examples above it is clear thta BERT model learned to pick up majority of the categories but very often missed one category from the true category list.  Good part Bert picks up majority of the classes, bad it often misses one class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics that would be useful for multilabel classification are F1 score and AUC/ROC curve. We will stick to F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.31552064418792725,\n",
       " 'eval_f1': 0.8050717570015328,\n",
       " 'eval_roc_auc': 0.8746604597581796,\n",
       " 'eval_accuracy': 0.276,\n",
       " 'eval_runtime': 7.1405,\n",
       " 'eval_samples_per_second': 140.045,\n",
       " 'eval_steps_per_second': 8.823,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'eval_loss': 0.31552064418792725,\n",
    " 'eval_f1': 0.8050717570015328,\n",
    " 'eval_roc_auc': 0.8746604597581796,\n",
    " 'eval_accuracy': 0.276,\n",
    " 'eval_runtime': 7.1405,\n",
    " 'eval_samples_per_second': 140.045,\n",
    " 'eval_steps_per_second': 8.823,\n",
    " 'epoch': 50.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## above we can observe performance measures of BERT model trained with 50 epochs for the test set with f1 score around 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047619047619047616"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To compare with random classifier we have 21 categories so random classifier which always predicts same category should be able to obtain around 5% accuracy while our model has 27.6% on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0571ec3060ae43b3811de5ccc85c9385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16538a192cb7482aabbaae4b633802e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d8b4662a5b4feba472c590f19a0f46",
      "placeholder": "",
      "style": "IPY_MODEL_5c4a113498a0434c905c86ff73b2abea",
      "value": "100%"
     }
    },
    "1c66085d538641d28d132d723c053fa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "1cd0041bd03f4028b421f7a44fc3c05e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ab9eb89743744a9aa717dd82a129201": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fba3a22993c4aedb1cf3bf745291cf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4082b8176278445997a2453706b7b101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0b9d1bfa51546f8a775488417d3a684",
       "IPY_MODEL_6c8eabd8b41f4cc4934c9c1f060026c6",
       "IPY_MODEL_ca0cf0adf2bb4653b04cd85c25d14a3e"
      ],
      "layout": "IPY_MODEL_1c66085d538641d28d132d723c053fa2"
     }
    },
    "431cea2a52604dd4910542e1fac2d045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_985db658d3614f1aabb57d173732f678",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec1f581ed92148b5bee7ce0441f8d883",
      "value": 5000
     }
    },
    "439053c658424b2f93a3f383b8d0af95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "45746650248241e1942e8809c94b06a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4f502d0988648d7996e1a55108a0fbe",
      "placeholder": "",
      "style": "IPY_MODEL_669f35f7f8fc4ad3b510972b19c69a3c",
      "value": "Map: 100%"
     }
    },
    "4a372b0842fa4a3e9aaf869bc2250cfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c4a113498a0434c905c86ff73b2abea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62471587c0c44a5182c4cb3033a78e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "669f35f7f8fc4ad3b510972b19c69a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66f61081b48d4f7785e1d150c3cb9a07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_686b34d4d82d4a5691fc6d4dfc51d0bb",
      "placeholder": "",
      "style": "IPY_MODEL_0571ec3060ae43b3811de5ccc85c9385",
      "value": " 5000/5000 [00:33&lt;00:00, 151.38 examples/s]"
     }
    },
    "686b34d4d82d4a5691fc6d4dfc51d0bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68abbd466ba64e8088b80c65bc0adb63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "6c8eabd8b41f4cc4934c9c1f060026c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e9849e134bd42cba950405613d6cee6",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_890e461de2bb4d2e902ca483b9c7b203",
      "value": 1000
     }
    },
    "73faac60485a45ef9234d3464b0b1c74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8901c5d6493f42fcb90992f2db6e402b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8f166d45b1c41d4b29e6cf63f5ed29d",
      "max": 11000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_439053c658424b2f93a3f383b8d0af95",
      "value": 11000
     }
    },
    "890e461de2bb4d2e902ca483b9c7b203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "91ae61f382f14883b9337f17dc8e7267": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "985db658d3614f1aabb57d173732f678": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e9849e134bd42cba950405613d6cee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1460512c5f1452e90bc943d2dbe1f1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16538a192cb7482aabbaae4b633802e5",
       "IPY_MODEL_c5d1139ec488496bb5ed2add54bbbbc3",
       "IPY_MODEL_b340d400a5a54da394eb030a89db63cd"
      ],
      "layout": "IPY_MODEL_3fba3a22993c4aedb1cf3bf745291cf7"
     }
    },
    "a4d8b4662a5b4feba472c590f19a0f46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adeb54cc7fa44b1f948595fb1e887926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afccdd194fec4ede9c65846cb0781d68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45746650248241e1942e8809c94b06a4",
       "IPY_MODEL_8901c5d6493f42fcb90992f2db6e402b",
       "IPY_MODEL_bca5bf694e1e44ce8be4e273f1c53ce2"
      ],
      "layout": "IPY_MODEL_68abbd466ba64e8088b80c65bc0adb63"
     }
    },
    "b340d400a5a54da394eb030a89db63cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c986f7a7cc7547adbc76007edc044c2c",
      "placeholder": "",
      "style": "IPY_MODEL_4a372b0842fa4a3e9aaf869bc2250cfc",
      "value": " 3/3 [00:00&lt;00:00, 91.72it/s]"
     }
    },
    "b3ff5b01b690434b8763f056165078c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4a289a28e694652b8e8d6d518314f63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9d01b97af884a198cfea00559c1f199",
       "IPY_MODEL_431cea2a52604dd4910542e1fac2d045",
       "IPY_MODEL_66f61081b48d4f7785e1d150c3cb9a07"
      ],
      "layout": "IPY_MODEL_91ae61f382f14883b9337f17dc8e7267"
     }
    },
    "bca5bf694e1e44ce8be4e273f1c53ce2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ab9eb89743744a9aa717dd82a129201",
      "placeholder": "",
      "style": "IPY_MODEL_73faac60485a45ef9234d3464b0b1c74",
      "value": " 11000/11000 [00:48&lt;00:00, 223.68 examples/s]"
     }
    },
    "c2582028d3ed41b9a32bc31df53a16dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f502d0988648d7996e1a55108a0fbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5d1139ec488496bb5ed2add54bbbbc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cd0041bd03f4028b421f7a44fc3c05e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62471587c0c44a5182c4cb3033a78e19",
      "value": 3
     }
    },
    "c986f7a7cc7547adbc76007edc044c2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca0cf0adf2bb4653b04cd85c25d14a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb31a2d5e23b465bb6abbdda03a17f02",
      "placeholder": "",
      "style": "IPY_MODEL_fe78160e25464f1589b6e09917c0f3f5",
      "value": " 1000/1000 [00:05&lt;00:00, 172.50 examples/s]"
     }
    },
    "d0b9d1bfa51546f8a775488417d3a684": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adeb54cc7fa44b1f948595fb1e887926",
      "placeholder": "",
      "style": "IPY_MODEL_b3ff5b01b690434b8763f056165078c0",
      "value": "Map: 100%"
     }
    },
    "d8f166d45b1c41d4b29e6cf63f5ed29d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9d01b97af884a198cfea00559c1f199": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2582028d3ed41b9a32bc31df53a16dd",
      "placeholder": "",
      "style": "IPY_MODEL_df37c400a9014907a887fce2a5fc7042",
      "value": "Map: 100%"
     }
    },
    "df37c400a9014907a887fce2a5fc7042": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec1f581ed92148b5bee7ce0441f8d883": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb31a2d5e23b465bb6abbdda03a17f02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe78160e25464f1589b6e09917c0f3f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
