{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe12771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MultiLabelBinarizer\n",
    "import nlp_preproc_functions as preproc\n",
    "import ast\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "os.chdir('/Users/jon/Documents/DSDM/term_2/adv_meth_nlp/nlp-final/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cf458c7",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d5ac19d",
   "metadata": {},
   "source": [
    "## Compare small and full dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fb6c11d",
   "metadata": {},
   "source": [
    "First load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English part of the dataset\n",
    "#This code breaks for me on my machine. I've \n",
    "train_dataset = load_dataset('multi_eurlex', 'en', split='train')\n",
    "test_dataset = load_dataset('multi_eurlex', 'en', split='test')\n",
    "val_dataset = load_dataset('multi_eurlex', 'en', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a45c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame([pd.to_datetime('2022-01-05'), pd.to_datetime('2021-5-23'), pd.to_datetime('13 octobre 1968'),pd.NaT])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37a3f341",
   "metadata": {},
   "source": [
    "`eurovoc_id` is the ID of a certain area of government activity.\n",
    "\n",
    "`level` refers to the specificity of a certain topic. For example, european parliament is level 0, quantum computing is level 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (label_id, descriptor) mapping \n",
    "with open('./data/eurovoc_descriptors.json') as jsonl_file:\n",
    "    eurovoc_descriptors =  json.load(jsonl_file)\n",
    "\n",
    "# Get feature map info\n",
    "train_classlabel = train_dataset.features[\"labels\"].feature\n",
    "test_classlabel = test_dataset.features[\"labels\"].feature\n",
    "val_classlabel = val_dataset.features[\"labels\"].feature\n",
    "\n",
    "# Load (label_id, descriptor) mapping \n",
    "with open('./data/eurovoc_concepts.json') as jsonl_file:\n",
    "    eurovoc_concepts =  json.load(jsonl_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb341d28",
   "metadata": {},
   "source": [
    "Let's take a look at the class imbalance for our predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, classlabel):\n",
    "    labels = [int(label) for label in sample['labels'].replace('[', '').replace(']', '').split()]\n",
    "    \n",
    "    results = [\n",
    "        {\n",
    "            'celex_id': sample['celex_id'],\n",
    "            'label_id': label_id,\n",
    "            'eurovoc_id': classlabel.int2str(label_id),\n",
    "            'eurovoc_desc': eurovoc_descriptors[classlabel.int2str(label_id)]['en'],\n",
    "            'eurovoc_level': next((level for level, ids in eurovoc_concepts.items() if classlabel.int2str(label_id) in ids), None)\n",
    "        }\n",
    "        for label_id in labels\n",
    "    ]\n",
    "    return results\n",
    "\n",
    "def get_agg_df(dataset, classlabel):\n",
    "    results = dataset.apply(process_sample, axis=1, classlabel=classlabel)\n",
    "    return pd.concat([pd.DataFrame(r) for r in results], ignore_index=True)\n",
    "\n",
    "# Read datasets\n",
    "train_dataset = pd.read_csv('./data/full_english_dataset/train_en')\n",
    "test_dataset = pd.read_csv('./data/full_english_dataset/test_en')\n",
    "val_dataset = pd.read_csv('./data/full_english_dataset/validation')\n",
    "\n",
    "train_sample = train_dataset.sample(frac=0.1)\n",
    "test_sample = test_dataset.sample(frac=0.1)\n",
    "val_sample = val_dataset.sample(frac=0.1)\n",
    "\n",
    "# Generate aggregated DataFrames\n",
    "train_agg_df = get_agg_df(train_sample, train_classlabel)\n",
    "test_agg_df = get_agg_df(test_sample, test_classlabel)\n",
    "val_agg_df = get_agg_df(val_sample, val_classlabel)\n",
    "\n",
    "# Concatenate the DataFrames and create a source column\n",
    "df = pd.concat([train_agg_df, test_agg_df, val_agg_df], ignore_index=True)\n",
    "df['source'] = pd.Series(['train'] * len(train_agg_df) + ['test'] * len(test_agg_df) + ['val'] * len(val_agg_df), dtype='string')\n",
    "\n",
    "# Group, pivot, and normalize the data\n",
    "df_pivot = df.groupby(['eurovoc_desc', 'source'])\\\n",
    "    .size()\\\n",
    "    .reset_index(name='count')\\\n",
    "    .pivot_table(index='eurovoc_desc', columns='source', values='count')\\\n",
    "    .divide(df_pivot.sum(axis=0), axis=1)\\\n",
    "    .sort_values(by='train', ascending=False)\n",
    "\n",
    "# Plot a grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "x = np.arange(len(df_pivot))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width, df_pivot['train'], width, label='train')\n",
    "ax.bar(x, df_pivot['test'], width, label='test')\n",
    "ax.bar(x + width, df_pivot['val'], width, label='val')\n",
    "\n",
    "ax.set_title('Occurrences of eurovoc_desc by source')\n",
    "ax.set_xlabel('eurovoc_desc')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.legend(title='Source', loc='upper right')\n",
    "\n",
    "xticks = x - width / 2 + 0.5\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(df_pivot.index, rotation=45, ha='right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3c92921",
   "metadata": {},
   "source": [
    "Note that this graph represents a 10% sample of our datasets, so we may not be able to make inferences about the balance of each label, but overall we can know that our dataset has class imbalance. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c428608",
   "metadata": {},
   "source": [
    "# Model and Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b63f3c6b",
   "metadata": {},
   "source": [
    "I haven't made the pipeline for preprocessing, bt here's a pipeline with the Logistic Regression model. But here are all the preprocessing steps in one place. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "519c106d",
   "metadata": {},
   "source": [
    "For our baseline modeling, we will evaluate the efficiency of three methods:\n",
    "* Random assignment\n",
    "* Word frequency\n",
    "* Logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "614645e6",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be215f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full dataframes\n",
    "df_train = pd.read_csv('./data/full_english_dataset/train_en')\n",
    "df_test = pd.read_csv('./data/full_english_dataset/test_en')\n",
    "df_val = pd.read_csv('./data/full_english_dataset/validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d74c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_train['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6f4a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test done\n"
     ]
    }
   ],
   "source": [
    "def process_all(df):\n",
    "    # fix labels\n",
    "    df['labels'] = df['labels'].apply(preproc.clean_label)\n",
    "    #get publishing date feature\n",
    "    df['pub_date'] = df['text'].apply(preproc.extract_date)\n",
    "    df = preproc.impute_timestamps(df, 'pub_date')\n",
    "    #apply standard preprocessing\n",
    "    df['pp_text'] = df['text'].apply(preproc.preprocess_text)\n",
    "    #add stopwords that occur once in in >=50% of documents\n",
    "    df = preproc.remove_common_words(df, 'pp_text', 'celex_id', threshold=0.5)\n",
    "    #Get labels as dummies\n",
    "    all_labels = set([label for labels in df['labels'] for label in labels])\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels_df = pd.DataFrame(mlb.fit_transform(df['labels']), columns=mlb.classes_)\n",
    "    labels_df = labels_df.add_prefix('label_')\n",
    "    df = pd.concat([df, labels_df], axis=1)\n",
    "    df = df.drop('labels', axis=1)\n",
    "    #get document type feature\n",
    "    df = preproc.get_eu_legal_type(df, 'text')\n",
    "    return df\n",
    "\n",
    "\n",
    "# read the full dataframes\n",
    "df_train = pd.read_csv('./data/full_english_dataset/train_en')\n",
    "df_test = pd.read_csv('./data/full_english_dataset/test_en')\n",
    "#df_val = pd.read_csv('./data/full_english_dataset/validation')\n",
    "\n",
    "\n",
    "#process\n",
    "df_test_pp = process_all(df_test)\n",
    "#print('test done')\n",
    "#df_val_pp = process_all(df_val)\n",
    "print('test done')\n",
    "df_train_pp = process_all(df_train)\n",
    "print('train done')\n",
    "\n",
    "#save\n",
    "df_train_pp.to_csv('train_pp.csv', index=False)\n",
    "df_test_pp.to_csv('test_pp.csv', index=False)\n",
    "#df_val_pp.to_csv('val_pp.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba75aaea",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa58d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./log_reg_model.pkl', 'rb') as file:\n",
    "    best_model = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fcd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('log_reg_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "# # Define the preprocessing steps for the textual data\n",
    "# text_preprocessor = Pipeline(steps=[\n",
    "#     ('count_vect', CountVectorizer(ngram_range=(1,2))), \n",
    "#     ('scale', StandardScaler(with_mean=False))\n",
    "#     ])\n",
    "\n",
    "# # Define the preprocessing steps for the categorical data\n",
    "# categorical_preprocessor = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "# # Define the column transformer to apply the different preprocessing steps to the different columns\n",
    "# preprocessor = ColumnTransformer(transformers=[\n",
    "#     ('text', text_preprocessor, 'pp_text'),\n",
    "#     ('date', StandardScaler(), ['pub_date']),\n",
    "#     ('categorical', categorical_preprocessor, ['commission', 'regulation', \n",
    "#                                                 'decision', 'council', 'directive', \n",
    "#                                                 'parliament', 'committee'])\n",
    "# ])\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_train_pp[['pp_text', 'pub_date', \n",
    "#                                                          'commission', 'regulation', 'decision', \n",
    "#                                                          'council', 'directive', 'parliament', \n",
    "#                                                          'committee']], \n",
    "#                                                     df_train_pp[['label_0', 'label_1', 'label_2', 'label_3', \n",
    "#                                                         'label_4', 'label_5', 'label_6', 'label_7', \n",
    "#                                                         'label_8', 'label_9', 'label_10', 'label_11', \n",
    "#                                                         'label_12', 'label_13', 'label_14', 'label_15', \n",
    "#                                                         'label_16', 'label_17', 'label_18', 'label_19', \n",
    "#                                                         'label_20']], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a pipeline with a placeholder for the classifier\n",
    "# clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                       ('classifier', None)])\n",
    "\n",
    "# # Set up the grid search\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'classifier': [MultiOutputClassifier(LogisticRegression(max_iter=500))],\n",
    "#     },\n",
    "#     {\n",
    "#         'classifier': [LogisticRegression(multi_class='multinomial', max_iter=500)],\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# grid_search = RandomizedSearchCV(clf, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model and its score\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_score = grid_search.best_score_\n",
    "# print('Best model:', best_model)\n",
    "# print('Best cross-validation accuracy: {:.2f}%'.format(best_score*100))\n",
    "\n",
    "# Evaluate the performance of the best model on the testing data\n",
    "# score = best_model.score(X_test, y_test)\n",
    "# print('Test accuracy: {:.2f}%'.format(score*100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a06b11b",
   "metadata": {},
   "source": [
    "Saved output from the above model:\n",
    "```python\n",
    "Best model: Pipeline(steps=[('preprocessor',\n",
    "                 ColumnTransformer(transformers=[('text',\n",
    "                                                  Pipeline(steps=[('count_vect',\n",
    "                                                                   CountVectorizer(ngram_range=(1,\n",
    "                                                                                                2))),\n",
    "                                                                  ('scale',\n",
    "                                                                   StandardScaler(with_mean=False))]),\n",
    "                                                  'pp_text'),\n",
    "                                                 ('date', StandardScaler(),\n",
    "                                                  ['pub_date']),\n",
    "                                                 ('categorical',\n",
    "                                                  Pipeline(steps=[('scaler',\n",
    "                                                                   StandardScaler())]),\n",
    "                                                  ['commission', 'regulation',\n",
    "                                                   'decision', 'council',\n",
    "                                                   'directive', 'parliament',\n",
    "                                                   'committee'])])),\n",
    "                ('classifier',\n",
    "                 MultiOutputClassifier(estimator=LogisticRegression(max_iter=500)))])\n",
    "Best cross-validation accuracy: 32.24%\n",
    "Test accuracy: 32.05%\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e000721a",
   "metadata": {},
   "source": [
    "Now let's evaluate its performance on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_pp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a753888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# Extract the input features and labels from the validation set\n",
    "X_val = df_val_pp[['pp_text', 'pub_date', \n",
    "                   'commission', 'regulation', 'decision', \n",
    "                   'council', 'directive', 'parliament', \n",
    "                   'committee']]\n",
    "y_val = df_val_pp[['label_0', 'label_1', 'label_2', 'label_3', \n",
    "                   'label_4', 'label_5', 'label_6', 'label_7', \n",
    "                   'label_8', 'label_9', 'label_10', 'label_11', \n",
    "                   'label_12', 'label_13', 'label_14', 'label_15', \n",
    "                   'label_16', 'label_17', 'label_18', 'label_19', \n",
    "                   'label_20']]\n",
    "# Get predictions from the model\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy\n",
    "# val_accuracy = accuracy_score(y_val, y_pred, average='micro')\n",
    "val_f1 = f1_score(y_val, y_pred, average='micro')\n",
    "# print('Validation accuracy: {:.2f}%'.format(val_accuracy*100))\n",
    "print('Validation F1: {:.2f}%'.format(val_f1*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0437614",
   "metadata": {},
   "source": [
    "Validation F1 is at 68.63%. Let's see which categories it predicts well, and which it predicts poorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_pred = pd.DataFrame(y_pred, columns=['pred_label_{}'.format(i) for i in range(y_pred.shape[1])])\n",
    "\n",
    "# Reset the indices of X_val, y_val, and df_y_pred\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "df_y_pred = df_y_pred.reset_index(drop=True)\n",
    "\n",
    "# Concatenate X_val, y_val, and df_y_pred along the columns\n",
    "df_results = pd.concat([X_val[['pp_text']], y_val, df_y_pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a88c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_desc = df.drop_duplicates(subset='label_id')[['label_id', 'eurovoc_desc']]\n",
    "labels_desc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two DataFrames on the label_id column\n",
    "df_merged = df_results.merge(labels_desc, left_on='label_{}'.format(i), right_on='label_id')\n",
    "\n",
    "# Calculate the F1 score for each label\n",
    "f1_scores = []\n",
    "for i in range(21):\n",
    "    score = f1_score(df_merged['label_{}'.format(i)], df_merged['pred_label_{}'.format(i)], average='micro')\n",
    "    f1_scores.append(score)\n",
    "\n",
    "# Get the eurovoc_desc for the x-axis\n",
    "labels = df_merged['eurovoc_desc'].unique()\n",
    "\n",
    "# Sort the F1 scores in descending order\n",
    "sorted_indices = np.argsort(f1_scores)[::-1]\n",
    "f1_scores_sorted = [f1_scores[i] for i in sorted_indices]\n",
    "labels_sorted = [labels[i] for i in sorted_indices]\n",
    "\n",
    "# Create a bar chart of the F1 scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels_sorted, f1_scores_sorted)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('eurovoc_desc')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Scores by eurovoc_desc (Sorted by Score)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00ee6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
